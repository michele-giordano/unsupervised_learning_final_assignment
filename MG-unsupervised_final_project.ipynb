{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860557f7",
   "metadata": {},
   "source": [
    "# Unsupervised Learning - Unsupervised Image Clustering comparison with PCA, NMF, Autoencoder and DEC\n",
    "by M. Giordano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4672b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is an Unsupervised Learning task focused on Image Clustering comparison with different unsupervised models.<br>\n",
    "The dataset in composed of various photorealistis natural AI generated images .<br>\n",
    "Our goal is to build a series of models to compare their ability to correctly clusterize into three groups: animals, birds and landscapes.\n",
    "\n",
    "This will be achieved through EDA, data cleaning, feature engineering, and model development from scratch.\n",
    "\n",
    "\n",
    "This work covers all the required points of the rubric, following the steps in the same order of appearance:\n",
    "\n",
    "1. Gather data, determine the method of data collection and provenance of the data (3 points)\n",
    "\n",
    "2. Identify an Unsupervised Learning Problem (6 points)\n",
    "\n",
    "3. Exploratory Data Analysis (EDA) — Inspect, Visualize, and Clean the Data (26 points)\n",
    "\n",
    "4. Perform Analysis Using Unsupervised Learning Models of your Choice, Present Discussion, and Conclusions (70 points)\n",
    "\n",
    "5. Produce Deliverables: High-Quality, Organized Jupyter Notebook Report, Video Presentation, and GitHub Repository (35 points)\n",
    "\n",
    "\n",
    "<br>\n",
    "This notebook, and the pdf version are available at the following Github repository:\n",
    "\n",
    "https://github.com/michele-giordano/unsupervised_learning_final_assignment/\n",
    "\n",
    "The video presentation can be seen at the following link:\n",
    "\n",
    "https://www.youtube.com/watch?v=fvw-nV2-CKw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421af63",
   "metadata": {},
   "source": [
    "## Step 1: Gather data, determine the method of data collection and provenance of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa14dc6",
   "metadata": {},
   "source": [
    "### 1.1 Data Collection and Provenance\n",
    "\n",
    "The dataset used in this work was created directly by the author, taking initial inspiration from image clustering competitions found on Kaggle.<br>\n",
    "During the review phase, I noticed that the datasets available there were generally of poor visual quality and highly inconsistent, often containing noisy, low-resolution, or poorly framed images.<br>\n",
    "These datasets reached a maximum accuracy values around 0.8 even under supervised learning conditions, indicating that the visual variability present was not informative but mostly chaotic.\n",
    "\n",
    "For this reason, they were not considered suitable for an unsupervised learning task where the structure of the data itself must guide the formation of meaningful clusters.\n",
    "\n",
    "To address this limitation, Stable Diffusion was used to generate a new dataset with a similar photographic style, but with controlled visual quality.<br>\n",
    "The images retain natural and realistic appearance, and include subjects that are not trivially separable, yet are clearly represented. This approach ensures a dataset that is coherent, reproducible, and appropriate for evaluating unsupervised image clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba083ec6",
   "metadata": {},
   "source": [
    "### 1.2 Method of Data Collection\n",
    "\n",
    "The images were generated with the explicit goal of representing the subjects in a structured and classifiable manner, while maintaining a high degree of heterogeneity within the dataset. More than five hundred images were created from scratch, including a variety of animal species, different types of birds, and diverse landscape scenarios across multiple seasons and lighting conditions. The intent was to build a dataset that is visually coherent but not uniform, allowing for meaningful variation along relevant visual features.\n",
    "\n",
    "Considering that the unsupervised methods used in this work will attempt to form clusters directly from pixel-level information, several images were deliberately designed to be challenging from a clustering perspective. Examples include polar bears against snowy backgrounds or brown-fur animals placed in forest environments, , where subject and background share very similar color distributions and silhouette is less distinct. \n",
    "\n",
    "These controlled difficulties were introduced to prevent the dataset from being trivially separable and to ensure that the clustering methods must effectively extract and leverage visual structure rather than relying on simple color or texture cues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ff092",
   "metadata": {},
   "source": [
    "## Step 2: Identify the Unsupervised Learning Problem\n",
    "\n",
    "<b>This is an unsupervised task of image clustering based on photorealistic images</b>.<br>\n",
    "The goal is to conduct EDA, perform data cleaning, extract meaningful visual features, and train a clustering model able to assign images to one of three conceptual groups: \n",
    "- animals, \n",
    "- birds, \n",
    "- landscapes. \n",
    "\n",
    "The model does not receive labeled data during training; instead, the structure must emerge from the visual patterns present in the images themselves.\n",
    "The evaluation therefore focuses on whether the resulting clusters form coherent visual groupings that align with these three categories, without relying on supervision or ground-truth labels.\n",
    "\n",
    "<b>This work goes through all the rubric requirements, implementing the necessary code entirely from scratch.</b><br>\n",
    "The analysis is designed to independently explore different approaches to feature extraction, dimensionality reduction, and clustering, rather than reproducing any existing kernels that could be found on Kaggle on similar tasks. \n",
    "\n",
    "Each stage of the pipeline, from preprocessing to model comparison, is developed and validated within this notebook to ensure methodological originality and full adherence to the rubric's expectations for analytical depth and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38af34f",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory Data Analysis (EDA) - Inspect, Visualize, and Clean the Data\n",
    "\n",
    "### 3.1 Describe the factors or components that make up the dataset \n",
    "\n",
    "As a first operation, we inspect the size and shape of the dataset to understand the nature of the collected data.<br>\n",
    "This step is essential to verify data integrity, identify potential irregularities, and gain an initial intuition about the structure of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba28010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score, silhouette_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce250e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exploratory Data Analysis (EDA) for RGB dataset ---\n",
    "\n",
    "UNLABELLED_DATASET = np.load(f\"data/dataset_256x256_unlabeled_shuffled.npz\", allow_pickle=True)\n",
    "LABELLED_DATASET = np.load(f\"data/dataset_256x256_labeled_shuffled.npz\", allow_pickle=True)\n",
    "\n",
    "data = UNLABELLED_DATASET\n",
    "images = data[\"X\"]\n",
    "X_raw = data[\"X\"]\n",
    "\n",
    "print(\"Total number of images:\", len(X_raw))\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for img in X_raw:\n",
    "    if img is None or img.ndim != 3:\n",
    "        continue\n",
    "    h, w = img.shape[:2]\n",
    "    heights.append(h)\n",
    "    widths.append(w)\n",
    "\n",
    "heights = np.array(heights)\n",
    "widths = np.array(widths)\n",
    "\n",
    "print(\"Minimum dimension:\", heights.min(), \"x\", widths.min())\n",
    "print(\"Maximum dimension:\", heights.max(), \"x\", widths.max())\n",
    "\n",
    "print(\"Number of distinct resolutions:\", len(set(zip(heights, widths))))\n",
    "\n",
    "# --- Visualize batches of RGB images ---\n",
    "def show_batch(images, start_index=0, batch_size=100):\n",
    "    end_index = min(start_index + batch_size, images.shape[0])\n",
    "    subset = images[start_index:end_index]\n",
    "\n",
    "    cols = 16\n",
    "    rows = int(np.ceil(batch_size / cols))\n",
    "    rows = cols\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, img in enumerate(subset):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(img.astype(np.uint8))\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(f\"Images {start_index}–{end_index-1}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: view first batch\n",
    "show_batch(X_raw, 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6c601",
   "metadata": {},
   "source": [
    "![EDA](images/eda.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcebedd",
   "metadata": {},
   "source": [
    "We can see that the dataset consists of 599 images with resolutions that range from  44 x 108 to 256 x 256.\n",
    "\n",
    "This diversity is intentional, as it focuses the attention on doing a proper data cleaning. Since the dataset is declared having 256x256, all other dimensions are considered noise to be removed. \n",
    "\n",
    "Each image can be represented as a three-dimensional matrix of pixel values, where the two spatial dimensions correspond to height and width, and the third dimension corresponds to the RGB color channels.\n",
    "\n",
    "These pixel values constitute the fundamental factors of the dataset, as all subsequent analysis steps, including feature extraction, dimensionality reduction, and clustering,  will be based on the relationships and variations within these numerical components.\n",
    "No additional metadata or external features are provided, ensuring that all learning emerges directly from the visual information contained in the pixel matrices.\n",
    "\n",
    "The graphical visualization of the first 100 records shows that this dataset is a collection of different natural photorealistic images, including birds, cats, dogs, and a variety of landscapes with different illumination and depicting diverse seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6fa63",
   "metadata": {},
   "source": [
    "### 3.2 Data Cleaning\n",
    "\n",
    "Firstly, we will remove all the images with resolution different than 256x256, then we will search for duplicates and remove them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e2431",
   "metadata": {},
   "source": [
    "![Data Cleaning](images/cleaned_dataset.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc9dfb",
   "metadata": {},
   "source": [
    "After the data cleaning phase, the dataset consists of 500 images, each standardized to a 256 × 256 resolution.\n",
    "The analysis of pixel intensities shows values ranging from 0 to 255, with a global mean of  123.635 and a standard deviation of 73.127.\n",
    "\n",
    "From the histograms of the RGB channels and the global pixel distribution, we can observe a balanced and well-spread color intensity across the three channels, with no saturation or clipping at the extremes.\n",
    "The distributions reveal a natural variability typical of photorealistic images, confirming that the dataset covers both bright and dark regions without dominant bias toward any single color component.\n",
    "This suggests that the dataset is visually diverse, properly normalized, and suitable for feature extraction and clustering in the following stages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11fc2a",
   "metadata": {},
   "source": [
    "### 3.3 Describe correlations between different factors of the dataset and justify your assumption that they are correlated or not correlated.\n",
    "\n",
    "In traditional tabular datasets, correlation analysis is typically used to detect redundant features that can be removed because they are linear combinations of others.\n",
    "\n",
    "In this case, however, <b>each feature corresponds to a fixed pixel position within a 256×256×3 image matrix.</b>\n",
    "The relationships among these features are primarily spatial rather than statistical: pixel values are not independent variables but elements of structured visual compositions. Adjacent pixels often share similar values due to color continuity within objects, while distant pixels may vary significantly depending on the spatial layout and content of the image.\n",
    "\n",
    "For this reason, a standard correlation matrix between pixel intensities would not yield meaningful insights about feature redundancy or independence. The observed dependencies are intrinsic to the geometry of the images rather than to the data distribution itself.\n",
    "\n",
    "Therefore, direct correlation analysis is not applied in this context. Instead, potential dependencies among features are implicitly captured and reduced through dimensionality reduction techniques such as PCA, NMF, or autoencoders, which are designed to extract compact latent representations that preserve the most relevant visual structure of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e8aa2",
   "metadata": {},
   "source": [
    "### 3.4 Determine if any data needs to be transformed.\n",
    "\n",
    "Since each record already consists of numerical pixel values, no mandatory transformations are required to make the dataset suitable for analysis.\n",
    "\n",
    "Nevertheless, certain transformations of the pixel values may be beneficial depending on the modeling approach adopted.\n",
    "\n",
    "These adjustments can improve numerical consistency and ensure that the representation of the images remains balanced across all features, allowing subsequent algorithms to operate on a coherent and comparable data scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41f04a",
   "metadata": {},
   "source": [
    "### 3.5 Using your hypothesis, indicate if it's likely that you should transform data\n",
    "\n",
    "Given that the dataset consists of RGB pixel matrices, no single transformation is universally required in advance.\n",
    "\n",
    "Normalization or standardization should be applied only when justified by the specific clustering method being employed, since different algorithms react differently to the scale of the input features.\n",
    "\n",
    "For distance-based methods such as PCA, KMeans, or linkage clustering, scaling procedures can be introduced to maintain numerical coherence and prevent individual channels or intensity ranges from dominating the computation. \n",
    "\n",
    "In contrast, models that operate directly on raw pixel structures or deep-learning-based feature extractors may not require preliminary transformations.\n",
    "\n",
    "Where appropriate, additional adjustments such as contrast correction, saturation control, or resolution changes may be introduced later to refine the consistency of the visual data and improve the quality of the extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8e777",
   "metadata": {},
   "source": [
    "### 3.6 You should determine if your data has outliers or needs to be cleaned in any way.\n",
    "\n",
    "As shown in the preliminary integrity checks performed in section 3.5, the dataset contains no missing, invalid, or inconsistent values.<br>All images share the same shape and data type, and no duplicated or corrupted records are present.\n",
    "\n",
    "Given that each record represents a real image, pixel variations cannot be interpreted as statistical outliers but rather as genuine visual differences among samples.\n",
    "\n",
    "Therefore, no data removal, interpolation, or substitution has been applied. The dataset can be considered clean and ready for use in the subsequent dimensionality reduction and clustering steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee976b61",
   "metadata": {},
   "source": [
    "### 3.7 If you believe that specific factors will be more important than others in your analysis, you should mention which and why. You will use this to confirm your intuitions in your final write-up.\n",
    "\n",
    "In this dataset, the most informative factors are not individual pixels but patterns of pixel intensities that describe shapes and textures within the images. \n",
    "\n",
    "These latent structures will be identified through matrix decomposition techniques such as PCA and NMF, which extract the underlying components that best represent visual variability. \n",
    "\n",
    "Their comparison will be performed in the next section to evaluate which representation provides more meaningful features for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56578e42",
   "metadata": {},
   "source": [
    "\n",
    "### 3.8 Balancing the dataset\n",
    "\n",
    "Considering that unsupervised models benefit from a balanced dataset, the three classes were equalized in size. \n",
    "\n",
    "The reduction in total samples is a reasonable trade-off, as methods like NMF, autoencoders, and DEC gain stability and clearer cluster structure when the class distribution is even, while PCA is the only method that remains largely unaffected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de2502f",
   "metadata": {},
   "source": [
    "## Step 4: Perform Analysis Using Unsupervised Learning Models of Your Choice, Present Discussion, and Conclusions\n",
    "\n",
    "In this section, we build and compare multiple models to analyze how different unsupervised learning techniques perform when applied to the photorealistic dataset created from scratch.\n",
    "\n",
    "The objective is to understand how each approach extracts latent features, organizes visual information, and forms clusters that reflect meaningful semantic structures within the images.\n",
    "\n",
    "While the focus of this work remains on unsupervised learning, a simple supervised baseline is also included to provide a reference point and to highlight the practical gap between self-organized and label-driven methods.\n",
    "\n",
    "The following models are considered:\n",
    "\n",
    "* **PCA + K-Means:** a linear projection combined with distance-based clustering, serving as a baseline for variance-driven separation.\n",
    "* **Hierarchical Linkage + NMF:** a hybrid approach that combines non-negative matrix factorization with hierarchical clustering to identify additive and interpretable visual components.\n",
    "* **Autoencoder:** a neural model trained to reconstruct input images through a compact latent representation, capturing non-linear dependencies among pixels.\n",
    "* **Deep Embedded Clustering (DEC):** an architecture that jointly optimizes representation learning and clustering to improve latent separability.\n",
    "* **DEC on Grayscale Images:** a variant used to assess the role of color information in cluster formation by training the DEC model on luminance-only inputs.\n",
    "* **Supervised Semantic Classifier (baseline):** a reference model trained on a small labeled subset to estimate the achievable upper-bound accuracy and to contextualize the limitations of unsupervised approaches.\n",
    "\n",
    "This multi-model comparison allows both quantitative and qualitative evaluation of clustering behavior, illustrating how increasing representational power affects the emergence of semantic structure.\n",
    "At the same time, the supervised baseline serves to confirm the internal coherence and overall quality of the dataset itself, showing that the lower accuracy of unsupervised methods reflects the intrinsic challenge of discovering structure without labels rather than any limitation of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8caaf",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In this report, clustering quality is evaluated using the following metrics, each capturing a different aspect of agreement between the predicted clusters and the ground-truth categories in the labelled_dataset.\n",
    "\n",
    "- ARI: Adjusted Rand Index measures the similarity between two partitions while correcting for chance. Higher values indicate stronger alignment between clusters and true labels.\n",
    "- NMI: Normalized Mutual Information quantifies how much information about the true labels is preserved by the clustering, scaled between 0 and 1 for comparability.\n",
    "- Hom: Homogeneity measures whether each cluster contains samples belonging to a single true class, penalizing mixed-class clusters.\n",
    "- Complt: Completeness evaluates whether all samples of a given true class are assigned to the same cluster, penalizing fragmentation.\n",
    "- V-Measure: The harmonic mean of homogeneity and completeness, providing a balanced summary of cluster purity and class cohesion.\n",
    "- Opt. Accuracy: Optimized accuracy aligns cluster IDs with true labels using the Hungarian algorithm, estimating the best possible accuracy achievable after resolving the arbitrary label permutations inherent in unsupervised clustering.\n",
    "\n",
    "In addition to the main metrics, we also examined the confusion matrix after applying the Hungarian alignment. This representation, however, is not always meaningful in unsupervised settings, especially when the learned clusters do not mirror the ground-truth distribution. \n",
    "\n",
    "For this reason, the evaluation focuses on permutation-invariant metrics described above, which provide a more reliable indication of the underlying partition quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea237ab1",
   "metadata": {},
   "source": [
    "### 4.1 PCA and K-Means\n",
    "\n",
    "\n",
    "The first model applies Principal Component Analysis (PCA) to reduce each image to a compact set of components that retain most of the variance, summarizing global color and texture while removing redundancy. \n",
    "\n",
    "On this representation, we apply K-Means to group images with similar latent features. Because centroid-based methods adapt well to the geometry of PCA-transformed data, K-Means is generally expected to perform better on images, especially when the main structures are captured by the leading components.\n",
    "\n",
    "As an alternative, we also apply hierarchical linkage clustering (Ward’s method) on the same PCA space. Unlike K-Means, linkage builds the cluster structure through irreversible merges that prioritize local variance minimization, offering a more rigid interpretation of the reduced feature space.\n",
    "\n",
    "Using these two PCA-based variants allows us to compare how flexible centroid-driven optimization and hierarchical merging behave when given the same linear embedding of the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf905ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Evaluate clustering vs ground truth (Category-based)\n",
    "# ============================================================\n",
    "def evaluate_clustering(labels_pred, labelled_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate clustering metrics against ground-truth categories.\n",
    "\n",
    "    Args:\n",
    "        labels_pred (array-like): predicted cluster labels\n",
    "        labelled_dataset (dict or np.lib.npyio.NpzFile): must contain 'Category'\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Extract ground-truth categories\n",
    "    # ------------------------------------------------------------\n",
    "    if isinstance(labelled_dataset, dict):\n",
    "        if \"Category\" in labelled_dataset:\n",
    "            labels_true = labelled_dataset[\"Category\"]\n",
    "        elif \"category\" in labelled_dataset:\n",
    "            labels_true = labelled_dataset[\"category\"]\n",
    "        else:\n",
    "            raise ValueError(\"LABELLED_DATASET must contain a 'Category' or 'category' key.\")\n",
    "    else:\n",
    "        # if it's a npz file\n",
    "        if hasattr(labelled_dataset, \"files\") and \"Category\" in labelled_dataset.files:\n",
    "            labels_true = labelled_dataset[\"Category\"]\n",
    "        elif hasattr(labelled_dataset, \"files\") and \"category\" in labelled_dataset.files:\n",
    "            labels_true = labelled_dataset[\"category\"]\n",
    "        else:\n",
    "            raise ValueError(\"LABELLED_DATASET must contain a 'Category' or 'category' key.\")\n",
    "\n",
    "    labels_true = np.array(labels_true)\n",
    "    labels_pred = np.array(labels_pred)\n",
    "\n",
    "    # safety check\n",
    "    if len(labels_true) != len(labels_pred):\n",
    "        min_len = min(len(labels_true), len(labels_pred))\n",
    "        print(f\"⚠️ Warning: length mismatch ({len(labels_true)} vs {len(labels_pred)}). Truncating to {min_len}.\")\n",
    "        labels_true = labels_true[:min_len]\n",
    "        labels_pred = labels_pred[:min_len]\n",
    "\n",
    "    # convert string categories to integer codes if necessary\n",
    "    if labels_true.dtype.type is np.str_ or labels_true.dtype == object:\n",
    "        unique_classes, encoded = np.unique(labels_true, return_inverse=True)\n",
    "        labels_true = encoded\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Metrics\n",
    "    # ------------------------------------------------------------\n",
    "    ari = adjusted_rand_score(labels_true, labels_pred)\n",
    "    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n",
    "    hom = homogeneity_score(labels_true, labels_pred)\n",
    "    comp = completeness_score(labels_true, labels_pred)\n",
    "    v_meas = v_measure_score(labels_true, labels_pred)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Optimized accuracy (Hungarian)\n",
    "    # ------------------------------------------------------------\n",
    "    n_classes = max(labels_true.max(), labels_pred.max()) + 1\n",
    "    contingency = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
    "    for t, p in zip(labels_true, labels_pred):\n",
    "        contingency[t, p] += 1\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency)\n",
    "    optimized_acc = contingency[row_ind, col_ind].sum() / contingency.sum()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Print results\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"\\n=== Clustering Evaluation vs Ground Truth (Category) ===\")\n",
    "    print(f\"Adjusted Rand Index (ARI):       {ari:.4f}\")\n",
    "    print(f\"Normalized Mutual Information:   {nmi:.4f}\")\n",
    "    print(f\"Homogeneity:                     {hom:.4f}\")\n",
    "    print(f\"Completeness:                    {comp:.4f}\")\n",
    "    print(f\"V-Measure:                       {v_meas:.4f}\")\n",
    "    print(f\"\\nOptimized Accuracy (Hungarian):  {optimized_acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"ARI\": ari,\n",
    "        \"NMI\": nmi,\n",
    "        \"Homogeneity\": hom,\n",
    "        \"Completeness\": comp,\n",
    "        \"V-Measure\": v_meas,\n",
    "        \"Optimized Accuracy\": optimized_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Generic 2D visualization \n",
    "# ===========================\n",
    "\n",
    "\n",
    "def plot_clusters_2d(X_2d, labels_pred, labels_true=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Display predicted clusters and true categories side by side from 2D features.\n",
    "\n",
    "    Args:\n",
    "        X_2d (np.ndarray): 2D feature array of shape (n_samples, 2)\n",
    "        labels_pred (array-like): predicted cluster labels\n",
    "        labels_true (array-like, optional): true category labels\n",
    "        model_name (str): model name to show in plot titles\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Sanity checks\n",
    "    if X_2d.shape[1] != 2:\n",
    "        pca_vis = PCA(n_components=2, random_state=42)\n",
    "        X_vis = pca_vis.fit_transform(X_2d)  # reduce to 2D just for visualaization\n",
    "        X_2d = X_vis\n",
    "\n",
    "    n_cols = 2 if labels_true is not None else 1\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=(10, 4))\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = [axes]\n",
    "\n",
    "    # --- Predicted clusters ---\n",
    "    ax = axes[0]\n",
    "    sc1 = ax.scatter(X_2d[:, 0], X_2d[:, 1], c=labels_pred, cmap=\"viridis\", s=8)\n",
    "    ax.set_title(f\"Predicted Clusters ({model_name})\", fontsize=10)\n",
    "    ax.set_xlabel(\"Component 1\", fontsize=8)\n",
    "    ax.set_ylabel(\"Component 2\", fontsize=8)\n",
    "    ax.tick_params(axis=\"both\", labelsize=8)\n",
    "\n",
    "    # --- True categories ---\n",
    "    if labels_true is not None:\n",
    "        enc = LabelEncoder()\n",
    "        y_true_encoded = enc.fit_transform(labels_true)\n",
    "        ax2 = axes[1]\n",
    "        sc2 = ax2.scatter(X_2d[:, 0], X_2d[:, 1], c=y_true_encoded, cmap=\"viridis\", s=8)\n",
    "        ax2.set_title(f\"True Categories\", fontsize=10)\n",
    "        ax2.set_xlabel(\"Component 1\", fontsize=8)\n",
    "        ax2.set_ylabel(\"Component 2\", fontsize=8)\n",
    "        ax2.tick_params(axis=\"both\", labelsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe31df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reduced to 200 components explaining 95.73% of variance\n",
      "Silhouette Score: 0.1785\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4.1 PCA + K-Means\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Flatten images and standardize\n",
    "# ------------------------------------------------------------\n",
    "X = images.reshape(len(images), -1).astype(np.float32)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Apply PCA for dimensionality reduction\n",
    "# ------------------------------------------------------------\n",
    "n_components = 100\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_var = np.sum(pca.explained_variance_ratio_) * 100\n",
    "print(f\"PCA reduced to {n_components} components explaining {explained_var:.2f}% of variance\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. K-Means clustering\n",
    "# ------------------------------------------------------------\n",
    "k = 3  # expected clusters: animals, birds, landscapes\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "labels_kmeans_pca = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Evaluate clustering\n",
    "# ------------------------------------------------------------\n",
    "sil_score = silhouette_score(X_pca, labels_kmeans_pca)\n",
    "print(f\"Silhouette Score: {sil_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc7831",
   "metadata": {},
   "source": [
    "![PCA](images/pca.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c225ddf",
   "metadata": {},
   "source": [
    "#### PCA: K-means vs Linkage Comparison Results\n",
    "\n",
    "The comparison of the evaluation metrics for both methods is summarised below:\n",
    "\n",
    "| Metric                              | PCA K-means | PCA Linkage (Ward) |\n",
    "| ----------------------------------- | ----------- | ------------------ |\n",
    "| Adjusted Rand Index (ARI)           | 0.490       | 0.525              |\n",
    "| Normalized Mutual Information (NMI) | 0.487       | 0.539              |\n",
    "| Homogeneity                         | 0.478       | 0.529              |\n",
    "| Completeness                        | 0.496       | 0.549              |\n",
    "| V-Measure                           | 0.487       | 0.539              |\n",
    "| Optimized Accuracy (Hungarian)      | 0.795       | 0.798              |\n",
    "\n",
    "\n",
    "As we can see and as expected, K-means adapts well to the geometry of the PCA-transformed space through its centroid-based optimisation. It captures the dominant variance directions and maintains a stable separation across the three categories. Hierarchical linkage behaves differently, but when the class densities are more uniform it benefits from a cleaner spatial layout and produces more consistent boundaries.\n",
    "\n",
    "PCA continues to encode global appearance reliably, and the interaction between the projection and the clustering algorithms becomes more balanced. Ward linkage, in particular, shows improved alignment with the structure induced by PCA and achieves metric values close to or slightly above those of K-means.\n",
    "\n",
    "Although both clustering methods produced very comparable accuracy on the ground-truth data, the linkage clustering has a higher ability to preserve coherent boundaries when the class distribution is uniform and the geometric structure is well defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23772f78",
   "metadata": {},
   "source": [
    "### 4.2 Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "NMF decomposes each image into additive, non-negative components that tend to highlight localized visual patterns such as color regions or texture fragments, offering a parts-based representation that differs substantially from the variance-oriented structure produced by PCA.\n",
    "\n",
    "After projecting the dataset into this feature space, we apply both clustering variants used throughout the analysis, allowing us to evaluate how the NMF decomposition influences the formation of groups under the same experimental conditions.\n",
    "\n",
    "This parallel treatment provides a direct comparison with the PCA-based models and helps determine whether the localized, interpretable features extracted by NMF lead to improvements in semantic organization across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Load labeled dataset\n",
    "# ---------------------------------------------------------\n",
    "X = images\n",
    "y_true = labelled_images[\"category\"]\n",
    "\n",
    "# Flatten images to 1D vectors\n",
    "X_flat = X.reshape(len(X), -1)\n",
    "\n",
    "# Scale to [0,1] for NMF stability\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_flat)\n",
    "\n",
    "# Encode string labels into integers\n",
    "le = LabelEncoder()\n",
    "y_true_int = le.fit_transform(y_true)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# NMF decomposition\n",
    "# ---------------------------------------------------------\n",
    "n_components = 100\n",
    "\n",
    "print(f\"Fitting NMF ({n_components} components)...\")\n",
    "nmf = None\n",
    "nmf = NMF(n_components=n_components, init='nndsvda', random_state=4, max_iter=100, verbose = True)\n",
    "X_nmf = nmf.fit_transform(X_scaled)\n",
    "\n",
    "# Clean any residual NaN or inf (should not appear, but safe)\n",
    "X_nmf = np.nan_to_num(X_nmf, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "X_nmf = X_nmf.astype(np.float64)\n",
    "\n",
    "print(\"NMF done. Shape of reduced data:\", X_nmf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f86ba6",
   "metadata": {},
   "source": [
    "![NMF](images/nmf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9825345",
   "metadata": {},
   "source": [
    "#### Hyperparameters tuning\n",
    "\n",
    "Different configurations of n_components, tol, random_state, and initialization methods were tested. The model proved highly sensitive to these parameters, with the most stable and consistent results obtained using 100 components, init='nndsvda', and random_seed = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020d2efa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Results of NMF K-Means vs Linkage (Ward)\n",
    "\n",
    "| Metric                              | NMF K-means | NMF Linkage (Ward) |\n",
    "| ----------------------------------- | ----------- | ------------------ |\n",
    "| Adjusted Rand Index (ARI)           | 0.036       | 0.726              |\n",
    "| Normalized Mutual Information (NMI) | 0.132       | 0.664              |\n",
    "| Homogeneity                         | 0.098       | 0.663              |\n",
    "| Completeness                        | 0.204       | 0.664              |\n",
    "| V-Measure                           | 0.132       | 0.664              |\n",
    "| Optimized Accuracy (Hungarian)      | 0.430       | 0.901              |\n",
    "\n",
    "\n",
    "Despite relying on a parts-based decomposition, the behaviour of the two clustering methods diverges sharply. K-Means shows limited agreement with the ground truth and highlights the difficulty of separating the NMF components using centroid-based criteria. The clusters remain diffuse and only loosely connected to the semantic structure of the dataset.\n",
    "\n",
    "Ward linkage, however, responds very differently once the NMF components are standardised. The hierarchical procedure identifies three compact regions with a level of consistency that is unusually strong for an unsupervised model of this type. All metrics rise substantially, and the optimized accuracy approaches values that are close to supervised performance.\n",
    "\n",
    "Overall, NMF provides a coherent latent space only when combined with scaling and hierarchical clustering. In this configuration, the results are unexpectedly strong and indicate that Ward linkage is able to capture the underlying structure with a clarity that K-Means does not achieve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596e916",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "### 4.3 Convolutional Autoencoder (PyTorch Implementation)\n",
    "\n",
    "We will implement this method using PyTorch Convolutional Autoencoder.\n",
    "\n",
    "This method differentiates from the previous ones by introducing a non-linear learning process.<br>\n",
    "The architecture consists of an encoder made of convolutional and max-pooling layers that progressively reduce the spatial resolution of the input images, and a decoder using transposed convolutions to reconstruct the original data from a compact latent representation. The network is trained in a fully unsupervised way using a reconstruction loss (MSE), optimized with the Adam algorithm and accelerated through CUDA.\n",
    "\n",
    "The main operations that will be performed are the following:\n",
    "\n",
    "- load the dataset through a custom Dataset class and process it with a DataLoader for batched training, resizing each image to 256×256 pixels and normalizing it to the [0,1] range;\n",
    "\n",
    "- define a convolutional autoencoder composed of an encoder with convolutional and max-pooling layers, and a decoder with transposed convolutions for image reconstruction;\n",
    "\n",
    "- train the model using the Mean Squared Error (MSE) loss and the Adam optimizer until reconstruction stability is achieved;\n",
    "\n",
    "- extract the latent feature vectors from the encoder and apply KMeans clustering to evaluate how well the learned representations capture the intrinsic structure of the dataset.\n",
    "\n",
    "We can expect an improvement in clustering performance, with the Adjusted Rand Index increasing by approximately 0.05–0.10 and the overall clustering accuracy reaching around 80–85%. The model is expected to learn a more coherent latent representation, enabling a clearer separation of semantic categories and a more accurate reconstruction of the underlying visual structures within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670de585",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "VERSION = 'v6'\n",
    "device = 'cuda'\n",
    "SEED = 4\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "images, labelled_images = load_dataset(\"combined_balanced_dataset_v6.npz\")\n",
    "\n",
    "\n",
    "# --- Filter and stack only valid images ---\n",
    "valid_images = []\n",
    "for img in images:\n",
    "    arr = np.array(img)\n",
    "    if arr.shape == (256, 256, 3):\n",
    "        valid_images.append(arr)\n",
    "    else:\n",
    "        print(f\"Skipping image with shape {arr.shape}\")\n",
    "\n",
    "images = np.stack(valid_images).astype(np.uint8)\n",
    "print(\"Final shape:\", images.shape)\n",
    "\n",
    "# Ensure 'images' is a clean numeric array\n",
    "if isinstance(images, list) or images.dtype == object:\n",
    "    # Convert each sub-array to np.uint8 and stack them\n",
    "    images = np.stack([np.array(img, dtype=np.uint8) for img in images])\n",
    "\n",
    "print(\"Images shape:\", images.shape, \"dtype:\", images.dtype)\n",
    "\n",
    "\n",
    "# Normalize and convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(images / 255.0, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "\n",
    "# Dummy labels (autoencoder is unsupervised)\n",
    "y_dummy = torch.zeros(len(X_tensor))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X_tensor, y_dummy)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Define Autoencoder model\n",
    "# ============================================================\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super().__init__()\n",
    "        # --- Encoder ---\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),   # 128x128\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 64x64\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), # 32x32\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),# 16x16\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*16*16, latent_dim)\n",
    "        )\n",
    "        # --- Decoder ---\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 256*16*16)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1), # 32x32\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),  # 64x64\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),   # 128x128\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),    # 256x256\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(self.decoder_fc(z).view(-1, 256, 16, 16))\n",
    "        return x_hat, z\n",
    "\n",
    "\n",
    "# Parameters (Setup A)\n",
    "max_epochs = 20\n",
    "patience = 20\n",
    "tolerance_decimal = 3\n",
    "latent_dim = 256\n",
    "lr = 0.001\n",
    "batch_size =  8\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "stable_count = 0\n",
    "last_epoch = -1\n",
    "last_loss = -1\n",
    "\n",
    "\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = ConvAutoencoder(latent_dim=latent_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# DataLoader (shuffle must stay False to preserve order)\n",
    "dataset = TensorDataset(X_tensor, y_dummy)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ============================================================\n",
    "# 5. Training loop with early stopping (4th decimal stability)\n",
    "# ============================================================\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{max_epochs}\"):\n",
    "        imgs = imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, _ = model(imgs)\n",
    "        loss = criterion(recon, imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{max_epochs} - AvgLoss: {avg_loss:.6f}\")\n",
    "\n",
    "    # Early stopping check (loss stable within 4 decimals)\n",
    "    if len(loss_history) > 1:\n",
    "        prev_loss = loss_history[-2]\n",
    "        if round(prev_loss, tolerance_decimal) == round(avg_loss, tolerance_decimal):\n",
    "            stable_count += 1\n",
    "        else:\n",
    "            stable_count = 0\n",
    "        if stable_count >= patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1} \"\n",
    "                  f\"(loss stable within 4 decimals for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "# ============================================================\n",
    "# 6. Save model\n",
    "# ============================================================\n",
    "\n",
    "#torch.save(model.state_dict(), f\"autoencoder_trained_from_images_{VERSION}_seed{SEED}_batch{batch_size}_epoch{max_epochs}_patience{patience}.pt\")\n",
    "#print(f\"✅ Model saved -> autoencoder_trained_from_images_{VERSION}_seed{SEED}_batch{batch_size}_epoch{max_epochs}_patience{patience}.pt\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. Extract embeddings, compute PCA (optional), KMeans and Linkage\n",
    "# ============================================================\n",
    "\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Encode all images\n",
    "all_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in DataLoader(dataset, batch_size=batch_size, shuffle=False):\n",
    "        imgs = imgs.to(device)\n",
    "        _, z = model(imgs)\n",
    "        all_embeddings.append(z.cpu().numpy())\n",
    "\n",
    "X_autoencoder = np.vstack(all_embeddings)  # shape: (N, latent_dim)\n",
    "\n",
    "# -------------------------\n",
    "# PCA on latent space (optional)\n",
    "# -------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50, random_state=4)\n",
    "X_PCA_autoencoder = pca.fit_transform(X_autoencoder)\n",
    "\n",
    "# -------------------------\n",
    "# KMeans\n",
    "# -------------------------\n",
    "from sklearn.cluster import KMeans\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=4)\n",
    "labels_kmeans_autoencoder = kmeans.fit_predict(X_autoencoder)\n",
    "\n",
    "# -------------------------\n",
    "# Linkage (Ward)\n",
    "# -------------------------\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "Z_autoencoder = linkage(X_autoencoder, method=\"ward\", metric=\"euclidean\")\n",
    "labels_linkage_autoencoder = fcluster(Z_autoencoder, t=3, criterion=\"maxclust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. Extract embeddings, compute PCA (optional), KMeans and Linkage\n",
    "# ============================================================\n",
    "\n",
    "# Switch to eval mode\n",
    "model = ConvAutoencoder(latent_dim=256).to(device)\n",
    "checkpoint = torch.load(\"autoencoder_trained_from_images_v6_seed4_batch8_epoch20_patience20-656_667_877_2.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "# Encode all images\n",
    "all_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in DataLoader(dataset, batch_size=batch_size, shuffle=False):\n",
    "        imgs = imgs.to(device)\n",
    "        _, z = model(imgs)\n",
    "        all_embeddings.append(z.cpu().numpy())\n",
    "\n",
    "X_autoencoder = np.vstack(all_embeddings)  # shape: (N, latent_dim)\n",
    "\n",
    "# -------------------------\n",
    "# PCA on latent space (optional)\n",
    "# -------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50, random_state=4)\n",
    "X_PCA_autoencoder = pca.fit_transform(X_autoencoder)\n",
    "\n",
    "# -------------------------\n",
    "# KMeans\n",
    "# -------------------------\n",
    "from sklearn.cluster import KMeans\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=4)\n",
    "labels_kmeans_autoencoder = kmeans.fit_predict(X_autoencoder)\n",
    "\n",
    "# -------------------------\n",
    "# Linkage (Ward)\n",
    "# -------------------------\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "Z_autoencoder = linkage(X_autoencoder, method=\"ward\", metric=\"euclidean\")\n",
    "labels_linkage_autoencoder = fcluster(Z_autoencoder, t=3, criterion=\"maxclust\")\n",
    "\n",
    "ari, nmi, acc = show_all_metrics (\"Autoencoder\", labels_kmeans_autoencoder, labels_linkage_autoencoder, X_autoencoder, Z_autoencoder, 50, k, y_true, le, images, labelled_images)\n",
    "suffix = str(acc)[2:][:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf71e4",
   "metadata": {},
   "source": [
    "![Autoencoder](images/autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e7fef",
   "metadata": {},
   "source": [
    "#### Autoencoder Results\n",
    "After an extensive series of tests combining seed, batch size, number of epochs, and patience, these metrics represent the best configuration obtained so far:\n",
    "- seed: 4\n",
    "- batch size: 8\n",
    "- max-epochs: 20\n",
    "- patience: 20\n",
    "\n",
    "The autoencoder produced a latent space that captures the dominant semantic structure of the dataset. Both clustering methods perform well on this representation, with linkage showing a stronger ability to separate the three visual groups. \n",
    "\n",
    "The results indicate that this configuration achieves a balanced embedding that supports stable, unsupervised discrimination of the classes.\n",
    "\n",
    "| Metric                              | AE K-Means | AE Linkage (Ward) |\n",
    "| ----------------------------------- | ---------- | ----------------- |\n",
    "| Adjusted Rand Index (ARI)           | 0.517      | 0.657             |\n",
    "| Normalized Mutual Information (NMI) | 0.515      | 0.667             |\n",
    "| Homogeneity                         | 0.507      | 0.660             |\n",
    "| Completeness                        | 0.523      | 0.675             |\n",
    "| V-Measure                           | 0.515      | 0.667             |\n",
    "| Optimized Accuracy (Hungarian)      | 0.804      | 0.877             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b58b7",
   "metadata": {},
   "source": [
    "### 4.4 Deep Embedded Clustering (DEC)\n",
    "\n",
    "Deep Embedded Clustering is an unsupervised method that integrates non-linear representation learning with a dedicated clustering objective. \n",
    "\n",
    "The process begins by training a convolutional autoencoder to construct a compact latent representation that preserves the essential visual structure of the input images. \n",
    "Once a stable reconstruction space is obtained, the decoder is removed and the encoder is further optimised using a clustering-driven loss.\n",
    "\n",
    "DEC initialises cluster centroids using KMeans in the latent space, then refines both the embeddings and the cluster assignments through an iterative procedure based on the Student’s t-distribution. This mechanism sharpens cluster boundaries by increasing the contrast between high-confidence assignments and ambiguous samples, gradually transforming the latent space into a more discriminative representation.\n",
    "\n",
    "This approach is particularly suitable for datasets such as the present one, where class separation depends on subtle shape regularities rather than simple colour statistics. The combination of non-linear feature extraction and clustering-oriented fine-tuning allows DEC to capture spatial structures that linear methods or purely reconstruction-based encoders tend to attenuate. As a result, DEC is well positioned to uncover meaningful semantic partitions even in moderately complex visual domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ba2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEEP EMBEDDED CLUSTERING (DEC) - Seed 4 Fine-Tuning\n",
    "# ============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Configuration\n",
    "# ------------------------------------------------------------\n",
    "images, labelled_images = load_dataset(\"combined_balanced_dataset_v6.npz\")\n",
    "\n",
    "VERSION = 6\n",
    "SEED = 4\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Running DEC on device: {device} (seed={SEED})\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Load hybrid dataset\n",
    "# ------------------------------------------------------------\n",
    "X = labelled_images[\"X\"]\n",
    "y_true = labelled_images[\"category\"]\n",
    "print(\"Dataset:\", X.shape)\n",
    "\n",
    "# Normalize to [0,1] and permute\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]                                # numpy uint8 [H,W,3]\n",
    "        img = torch.tensor(img, dtype=torch.float32)     # float32\n",
    "        img = img.permute(2,0,1) / 255.0                 # to [0,1], CHW\n",
    "        return img\n",
    "        \n",
    "\n",
    "dataset = ImageDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Encoder definition (must match previous architecture)\n",
    "# ------------------------------------------------------------\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 16 * 16, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Load pretrained encoder\n",
    "encoder = ConvEncoder().to(device)\n",
    "checkpoint_full = torch.load(\"autoencoder_trained_from_images_v6_seed4_batch8_epoch20_patience20-656_667_877_2.pt\", map_location=device)\n",
    "\n",
    "# Adapt keys if they do not have \"encoder.\" prefix\n",
    "state_dict = {}\n",
    "for k, v in checkpoint_full.items():\n",
    "    if k.startswith(\"encoder.\"):\n",
    "        new_key = k.replace(\"encoder.\", \"\")\n",
    "        state_dict[new_key] = v\n",
    "    elif any(s in k for s in [\"0.weight\", \"0.bias\", \"2.weight\", \"2.bias\"]):\n",
    "        # direct weights from encoder (no prefix)\n",
    "        state_dict[k] = v\n",
    "\n",
    "encoder.load_state_dict(state_dict, strict=False)\n",
    "encoder.eval()\n",
    "print(\"Pretrained encoder weights loaded successfully (seed 4).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291dda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. DEC Training Loop (Android-Safe)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def soft_assign(z, centroids, alpha=1):\n",
    "    \"\"\"Compute soft assignment q_ij based on Student t-distribution.\"\"\"\n",
    "    dist = torch.cdist(z, centroids, p=2) ** 2\n",
    "    numerator = (1.0 + dist / alpha) ** (-(alpha + 1) / 2)\n",
    "    q = numerator / torch.sum(numerator, dim=1, keepdim=True)\n",
    "    return q\n",
    "\n",
    "def target_distribution(q):\n",
    "    \"\"\"Sharpen the soft assignments to get target distribution p.\"\"\"\n",
    "    weight = q ** 2 / torch.sum(q, dim=0)\n",
    "    p = (weight.t() / torch.sum(weight, dim=1)).t()\n",
    "    return p\n",
    "\n",
    "def metrics_on_the_fly(checkpoint):\n",
    "    # Remap keys: \"0.weight\" → \"encoder.0.weight\"\n",
    "    remapped = {}\n",
    "    for k, v in checkpoint.items():\n",
    "        new_key = f\"encoder.{k}\"\n",
    "        remapped[new_key] = v\n",
    "\n",
    "    encoder = ConvEncoder().to(device)\n",
    "    encoder.load_state_dict(remapped, strict=False)\n",
    "    encoder.eval()\n",
    "\n",
    "    features_dec = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm(dataloader, desc=\"Extracting final latent features\"):\n",
    "            imgs = imgs.to(device)\n",
    "            z = encoder(imgs)\n",
    "            features_dec.append(z.cpu())\n",
    "    features_dec = torch.cat(features_dec).numpy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features_dec = scaler.fit_transform(features_dec)\n",
    "    kmeans_refined = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=1)\n",
    "    #labels_kmeans_dec = kmeans_refined.fit_predict(features_dec)\n",
    "\n",
    "    # DEC LINKAGE\n",
    "\n",
    "    # 1. Usa già features_dec normalizzate e scalate\n",
    "    X_dec = features_dec  # shape [N, latent_dim] dopo StandardScaler\n",
    "\n",
    "    # 2. Linkage Ward\n",
    "    Z_linkage_dec = linkage(X_dec, method=\"ward\")\n",
    "\n",
    "    # 3. Cut-tree per ottenere k cluster\n",
    "    labels_linkage_dec = fcluster(Z_linkage_dec, t=3, criterion=\"maxclust\")\n",
    "\n",
    "    # 4. Valutazione\n",
    "    ari = adjusted_rand_score(y_true, labels_linkage_dec)\n",
    "    nmi = normalized_mutual_info_score(y_true, labels_linkage_dec)\n",
    "    v = v_measure_score(y_true, labels_linkage_dec)\n",
    "\n",
    "    print(f\"DEC + Linkage Ward | ARI={ari:.4f} | NMI={nmi:.4f} | V={v:.4f}\")\n",
    "    return ari,nmi,v\n",
    "\n",
    "SEED = 4\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "MAX_EPOCHS =200\n",
    "PATIENCE = 100\n",
    "DELTA = 1e-4\n",
    "ALPHA = 1  # Student t-distribution parameter\n",
    "\n",
    "encoder = ConvEncoder().to(device)\n",
    "checkpoint = torch.load(f\"AE_undertrained_seed{SEED}_{device}_v{VERSION}.pt\", map_location=device)\n",
    "\n",
    "# Remap keys: \"0.weight\" → \"encoder.0.weight\"\n",
    "remapped = {}\n",
    "for k, v in checkpoint.items():\n",
    "    new_key = f\"encoder.{k}\"\n",
    "    remapped[new_key] = v\n",
    "\n",
    "encoder = ConvEncoder().to(device)\n",
    "encoder.load_state_dict(remapped, strict=False)\n",
    "encoder.eval()\n",
    "\n",
    "print(\"Undertrained encoder loaded successfully and mapped to ConvEncoder.\")\n",
    "\n",
    "encoder.eval()\n",
    "print(\"Pretrained encoder weights loaded successfully (seed 4).\")\n",
    "\n",
    "\n",
    "initial_centroids = torch.tensor(kmeans_init.cluster_centers_, dtype=torch.float32).to(device)\n",
    "\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=1e-4)\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "last_epoch = 0\n",
    "\n",
    "import copy\n",
    "\n",
    "best_ari = -1\n",
    "best_nmi = -1\n",
    "best_state = None\n",
    "best_epoch = -1\n",
    "\n",
    "prev_loss = None\n",
    "loss_descent = 0\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # ----------------------------------------------\n",
    "    # STEP 1: Extract all latent vectors (z)\n",
    "    # ----------------------------------------------\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        z_full = []\n",
    "        for imgs in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            z_full.append(encoder(imgs))\n",
    "        z_full = torch.cat(z_full)  # shape [N, latent_dim]\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # STEP 2: Compute q and p over full dataset\n",
    "    # ----------------------------------------------\n",
    "    q = soft_assign(z_full, initial_centroids)\n",
    "    p = target_distribution(q)  # shape [N, k]\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # STEP 3: Update centroids (critical fix)\n",
    "    # ----------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        numer = torch.matmul(q.t(), z_full)         # shape [k, latent_dim]\n",
    "        denom = q.sum(dim=0, keepdim=True).t()      # shape [k, 1]\n",
    "        updated_centroids = numer / denom           # shape [k, latent_dim]\n",
    "        initial_centroids = updated_centroids       # overwrite\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # STEP 4: Train encoder using mini-batches\n",
    "    # ----------------------------------------------\n",
    "    encoder.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_idx, imgs in enumerate(dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "        z = encoder(imgs)\n",
    "        q_batch = soft_assign(z, initial_centroids)\n",
    "        \n",
    "        start = batch_idx * imgs.size(0)\n",
    "        end = start + imgs.size(0)\n",
    "        p_batch = p[start:end]\n",
    "\n",
    "        loss = criterion(torch.log(q_batch + 1e-8), p_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    if prev_loss is None:\n",
    "        prev_loss = 1000\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} - KL Loss: {avg_loss:.6f}\")\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Optional evaluation (unchanged)\n",
    "    # ----------------------------------------------\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        z_eval = []\n",
    "        for imgs in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            z_eval.append(encoder(imgs))\n",
    "        z_eval = torch.cat(z_eval).cpu().numpy()\n",
    "\n",
    "    ari_t,nmi_t,v_t = metrics_on_the_fly(encoder.state_dict())\n",
    "\n",
    "\n",
    "    print(f\"                                  [Progress @Epoch {epoch+1}] ARI={ari_t:.4f} | NMI={nmi_t:.4f} | V-Measure={v_t}\" )\n",
    "    # Track best model based on ARI (or NMI)\n",
    "    if prev_loss > avg_loss:\n",
    "        loss_descent += 1\n",
    "        print(f\"                                                       DESCENT: {loss_descent}\")\n",
    "\n",
    "#    if ari_t > best_ari:  # or use nmi_t if preferred\n",
    "    best_ari = ari_t\n",
    "    best_nmi = nmi_t\n",
    "    best_epoch = epoch + 1\n",
    "    best_state = copy.deepcopy(encoder.state_dict())\n",
    "    print(f\"                                                       >>> NEW BEST (epoch {best_epoch}) ARI={best_ari:.4f} NMI={best_nmi:.4f}\")\n",
    "    torch.save(best_state, f\"DEC_encoder_best_seed{SEED}_{device}_{VERSION}_bestepoch_{best_epoch}_{best_ari:.3f}_{best_nmi:.3f}_alpha{ALPHA}_batch{BATCH_SIZE}.pt\")\n",
    "    print(f\"                                                           saved @ epoch {best_epoch} -> DEC_encoder_best_seed{SEED}_{device}_{VERSION}_bestepoch_{best_epoch}_{best_ari:.3f}_{best_nmi:.3f}_alpha{ALPHA}_batch{BATCH_SIZE}.pt\")\n",
    "\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # Early stopping\n",
    "    # ----------------------------------------------\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    last_epoch = epoch\n",
    "    prev_loss = avg_loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4e221",
   "metadata": {},
   "source": [
    "![DEC](images/dec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b72a35",
   "metadata": {},
   "source": [
    "#### DEC Results\n",
    "\n",
    "| Metric                              | DEC K-Means | DEC Linkage (Ward) |\n",
    "| ----------------------------------- | ----------- | ------------------ |\n",
    "| Adjusted Rand Index (ARI)           | 0.440       | 0.684              |\n",
    "| Normalized Mutual Information (NMI) | 0.471       | 0.645              |\n",
    "| Homogeneity                         | 0.460       | 0.644              |\n",
    "| Completeness                        | 0.482       | 0.646              |\n",
    "| V-Measure                           | 0.471       | 0.645              |\n",
    "| Optimized Accuracy (Hungarian)      | 0.643       | 0.883              |\n",
    "\n",
    "\n",
    "As we can see from the above metrics, DEC with K-Means clustering does not provide a meaningful improvement, which is not surprising given that in this particular scenario the RGB images are not easily separable by simple centroid-based boundaries. \n",
    "Instead, <b>the linkage-based approach delivers a clear performance gain<b>, reaching ARI 0.684, NMI 0.645 and an optimized <b>accuracy of 0.883</b>, the highest of all the methods analyzed. \n",
    "\n",
    "This indicates that the latent space learned by DEC does contain discriminative structure, but its topology is better captured by hierarchical relationships than by fixed-radius partitions. \n",
    "\n",
    "We can conclude that when the visual differences between classes are subtle and distributed, like in the dataset of images of this work, the linkage clustering appears to exploit the embedding more effectively, producing a separation that is both robust and semantically coherent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296bed0",
   "metadata": {},
   "source": [
    "### 4.5 Supervised baseline (SVC classifier)\n",
    "\n",
    "Finally, we introduce a supervised SVC model to estimate the maximum accuracy that can be realistically achieved on this dataset. \n",
    "\n",
    "As the title suggests, this experiment is included purely as a reference point, allowing us to assess the intrinsic limitations of unsupervised learning and quantify the gap between our best clustering results and a label-driven approach. \n",
    "\n",
    "A Support Vector Classifier is particularly suitable in this setting, as it performs well in high-dimensional continuous feature spaces and has a long record of effectiveness on image data transformed through latent representations or dimensionality reduction. \n",
    "\n",
    "The aim is not to optimize the supervised model, but to establish a credible upper bound that validates the dataset’s separability and frames the performance of the unsupervised techniques in a meaningful way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19028719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised SVC Accuracy: 0.942\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     animals       0.93      0.89      0.91        28\n",
      "       birds       0.96      0.93      0.95        29\n",
      "   landscape       0.94      1.00      0.97        29\n",
      "\n",
      "    accuracy                           0.94        86\n",
      "   macro avg       0.94      0.94      0.94        86\n",
      "weighted avg       0.94      0.94      0.94        86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Supervised Baseline – SVC Classifier (3 Categories)\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Extract features and labels\n",
    "# ------------------------------------------------------------\n",
    "X = labelled_images[\"X\"]               # shape (N, features)\n",
    "# Flatten to (N, features)\n",
    "X = X.reshape(len(X), -1)\n",
    "\n",
    "y = labelled_images[\"category\"]\n",
    "\n",
    "# Encode categories into integers 0/1/2\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(y)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Train/test split\n",
    "# ------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_int, test_size=0.25, random_state=42, stratify=y_int\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Train SVC\n",
    "# ------------------------------------------------------------\n",
    "clf = SVC(kernel=\"rbf\", C=2, gamma=\"scale\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Evaluation\n",
    "# ------------------------------------------------------------\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Supervised SVC Accuracy: {acc:.3f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6c162",
   "metadata": {},
   "source": [
    "#### supervised SVC results\n",
    "\n",
    "Supervised SVC Accuracy: 0.942\n",
    "<pre>\n",
    "Classification Report:\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "animals         0.93      0.89      0.91        28\n",
    "birds           0.96      0.93      0.95        29\n",
    "landscape       0.94      1.00      0.97        29\n",
    "\n",
    "accuracy                            0.94        86\n",
    "macro avg       0.94      0.94      0.94        86\n",
    "weighted avg    0.94      0.94      0.94        86\n",
    "</pre>\n",
    "A brief comparison with the supervised baseline confirms that the dataset is internally consistent and sufficiently informative, as a simple SVC reaches an accuracy of approximately 0.94 without any specialized optimization.\n",
    "\n",
    "The lower performance of the unsupervised models was expected, since they operate without labels and rely purely on latent structure.\n",
    "\n",
    "However, the relatively small gap of about six percentage points between the best DEC + linkage configuration (~0.88) and the supervised reference (~0.94) shows that the unsupervised approach is far from ineffective. \n",
    "\n",
    "On the contrary, it demonstrates a meaningful capacity to recover semantic groupings directly from the data, without relying on annotated examples, and therefore remains a viable option when labels are costly or unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759084d",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this project we explored a hierarchy of unsupervised representation and clustering strategies on an RGB image dataset with three semantic categories. Starting from basic linear decompositions, and gradually moving toward more advanced neural approaches, we built a clear trajectory: **PCA → NMF → Autoencoder → DEC Fine-Tuned (RGB)**. \n",
    "\n",
    "All techniques were applied using only the raw RGB channels as input, and the comparison across K-Means and linkage clustering revealed how different methods extract structure from the same visual signal, highlighting the relationship between latent representation quality, clustering behavior, and computational efficiency.\n",
    "\n",
    "The following table shows the metrics collected during this work:\n",
    "\n",
    "| Model                   | Configuration                               | Accuracy (%) | ARI   | NMI   | V-Measure | Notes                                                       |\n",
    "| ----------------------- | ------------------------------------------- | ------------ | ----- | ----- | --------- | ----------------------------------------------------------- |\n",
    "| PCA + K-Means           | n_components = 200                          | 79.5         | 0.490 | 0.487 | 0.487     | Solid baseline, acceptable performance                      |\n",
    "| PCA + Linkage           | n_components = 200                          | 79.8         | 0.525 | 0.539 | 0.539     | Slight improvement over K-Means                             |\n",
    "| NMF + K-Means           | n_components=100, max_iter=100, normalized  | 43.0         | 0.036 | 0.132 | 0.132     | Very poor separation with K-Means                           |\n",
    "| NMF + Linkage           | n_components=100, max_iter=100, normalized  | <b>90.1</b>  | 0.726 | 0.664 | 0.664     | <b>Highest overall metrics among all models</b>             |\n",
    "| Autoencoder + K-Means   | epochs=20, patience=20, batch=8             | 60.8         | 0.387 | 0.412 | 0.412     | Moderate results, clearly better than NMF                   |\n",
    "| Autoencoder + Linkage   | epochs=20, patience=20, batch=8             | 87.7         | 0.657 | 0.667 | 0.667     | Strong performance, close to supervised                     |\n",
    "| DEC + K-Means           | max_epoch=110, patience=30, alpha=1.0       | 64.3         | 0.440 | 0.471 | 0.471     | Better than AE+KMeans, weaker than linkage                  |\n",
    "| DEC + Linkage           | max_epoch=110, patience=30, alpha=1.0       | 88.3         | 0.684 | 0.645 | 0.645     | Best model for mobile and GPU implementation                |\n",
    "| Supervised SVC          | C=2, test_size=0.25                         | 94.2         |       |       |           | Precision 0.93–0.96, Recall 0.89–1.00, F1 up to 0.97        |\n",
    "\n",
    "\n",
    "In summary, the results confirm that even when restricted to raw RGB information, unsupervised learning can recover a meaningful semantic structure from image data. \n",
    "\n",
    "The progression from PCA to NMF, to neural embeddings with autoencoder, and finally to DEC illustrates how increasingly expressive representations affect cluster separability, with linkage-based methods generally outperforming K-Means across all models.\n",
    "\n",
    "The results also show that, with targeted tuning and careful selection of parameters and hyperparameters, each method can achieve meaningful performance, with accuracy values ranging approximately from 79% to over 90% depending on the model and clustering strategy.\n",
    "\n",
    "<b>NMF combined with hierarchical clustering achieved the best overall metrics</b>, demonstrating that simplicity and interpretability do not necessarily limit effectiveness. \n",
    "\n",
    "Neural approaches, while more costly to train, offered competitive results and showed particular robustness when using linkage clustering, narrowing the gap with the supervised reference model.\n",
    "\n",
    "From an operational perspective, the choice of model also depends on deployment constraints. While NMF remains the most accurate in absolute terms, DEC provides a compact neural encoder compatible with PyTorch Mobile and GPU execution, making it the most practical option for an Android demonstration where lightweight inference and library support are essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e5dd8",
   "metadata": {},
   "source": [
    "## Android Implementation Demo\n",
    "\n",
    "An Android demo app is being developed to demonstrate the practical potential of the DEC-based unsupervised training.\n",
    "\n",
    "The application uses the device’s camera to perform **real-time inference**, detecting whether the object or scene currently in the frame corresponds to one of the three discovered clusters: \n",
    "- *animal*,\n",
    "- *bird*, \n",
    "- or *landscape*.\n",
    "\n",
    "This lightweight mobile implementation highlights how an unsupervised model, once trained, can be deployed efficiently on edge devices without the need for labeled data or cloud computation.\n",
    "\n",
    "You can see the demo in action at the following link:\n",
    "https://www.youtube.com/watch?v=fvw-nV2-CKw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
